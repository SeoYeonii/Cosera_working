{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "Using TensorFlow backend.\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n17063936/17464789 [============================>.] - ETA: 0s"
                }
            ], 
            "source": "#Loading IMDB sentiment data\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding\nfrom keras.layers import LSTM\nfrom keras.datasets import imdb\n\nmax_features = 20000\nmaxlen = 80\n\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#Padding sequences and defining LSTM model\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n# if the sequence training and testing of data sets is shorter than 80 items, \n# we would pad them with zeros at the end, and if they're longer, we crack them. \n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 128))\nmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Train on 25000 samples, validate on 25000 samples\nEpoch 1/15\n25000/25000 [==============================] - 156s - loss: 0.6931 - acc: 0.5054 - val_loss: 0.6930 - val_acc: 0.5156\nEpoch 2/15\n25000/25000 [==============================] - 153s - loss: 0.6929 - acc: 0.5138 - val_loss: 0.6927 - val_acc: 0.5314\nEpoch 3/15\n24928/25000 [============================>.] - ETA: 0s - loss: 0.6927 - acc: 0.5215"
                }
            ], 
            "source": "# Run and evaluate model\nmodel.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n# which we again do with binary crossentropy, suppress the gradient and we also have the same accuracy as before.\n\nmodel.fit(x_train, y_train, batch_size=32, epochs=15, validation_data=(x_test, y_test))\n\nmodel.evaluate(x_test, y_test, batch_size=32)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}
