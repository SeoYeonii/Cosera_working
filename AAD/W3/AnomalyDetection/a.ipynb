{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "## Anomaly Detection\n\nSince this is unsupervised machine learning, we don't have labels available. <br  />\nSo the task of a neural network is to take X and adjust the ways that it reconstructs Y,<br  />\nbut we don't have Y. <br  />\nSo hopefully, you remember what the outer encoder does. <br  />\nSo an outer encoder is using X as input and output as well. <br  />\nSo an outer encoder will try to reconstruct what it sees on the left-hand side on the right-hand side. <br  />\nAnd if you now train the outer encoder, with an LSTM and healthy data, it tries to reconstruct the healthy data. <br  />\nAnd remember that it runs through a neural bottleneck.<br  />\nAnd once it's trained, it will have a hard time to reconstruct faulty data, that's the point.<br  />\nThat's how the anomaly detector works.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "### * An LSTM Auto-encoder based Anomaly Detector in Keras\n\uc548\ub41c\ub2e4 \uc800 \ud398\uc774\uc9c0\uc5d0 \uc548 \ub4e4\uc5b4\uac00\uc9d0", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import numpy as np\nimport pandas as pd\nimport scipy.io as sio\nimport os", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "!wget http://csegroups.case.edu/sites/default/files/bearingdatacenter/files/Datafiles/97.mat\n!wget http://csegroups.case.edu/sites/default/files/bearingdatacenter/files/Datafiles/98.mat\n!wget http://csegroups.case.edu/sites/default/files/bearingdatacenter/files/Datafiles/99.mat\n!wget http://csegroups.case.edu/sites/default/files/bearingdatacenter/files/Datafiles/100.mat", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "!mkdir  cwr_healthy", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "!mv *.mat cwr_healthy/", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "def read_folder(folder):\n    data = 'dummy'\n    skip = False\n    for file in os.listdir(folder):\n        file_id = file[:-4]\n        mat_file_dict = sio.loadmat(folder+file)\n        del data\n        for key, value in mat_file_dict.items():\n            if 'DE_time' in key or 'FE_time' in key:\n                a = np.array(mat_file_dict[key])\n                try:\n                    data\n                except NameError:\n                    data = a\n                else:\n                    if (data.shape[0] != a.shape[0]):\n                        print 'skipping ' + file_id\n                        skip = True\n                        continue\n                    data = np.hstack((data,a))\n        if skip:\n            skip=False\n            continue\n        id = np.repeat(file_id,data.shape[0])\n        id.shape = (id.shape[0],1)\n        data = np.hstack((id,data))\n        if data.shape[1] == 2:\n            zeros = np.repeat(float(0),data.shape[0])\n            zeros.shape =(data.shape[0],1)\n            data = np.hstack((data,zeros))\n        try:\n            result\n        except NameError:\n            result = data\n        else:\n            result = np.vstack((result,data))\n    return result", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "result_healthy = read_folder('./cwr_healthy/')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "\npdf = pd.DataFrame(result_healthy)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pdf.to_csv('result_healthy_pandas.csv', header=False, index=True)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "!for url in `curl -s csegroups.case.edu/bearingdatacenter/pages/12k-drive-end-bearing-fault-data |grep mat |grep http |awk -F'href=\"' '{print $2}' |awk -F'\">' '{print $1}'`; do wget $url; done\n!for url in `curl -s csegroups.case.edu/bearingdatacenter/pages/48k-drive-end-bearing-fault-data |grep mat |grep http |awk -F'href=\"' '{print $2}' |awk -F'\">' '{print $1}'`; do wget $url; done\n!for url in `curl -s csegroups.case.edu/bearingdatacenter/pages/12k-fan-end-bearing-fault-data |grep mat |grep http |awk -F'href=\"' '{print $2}' |awk -F'\">' '{print $1}'`; do wget $url; done\n!mkdir cwr_faulty\n!mv *.mat cwr_faulty/", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "result_faulty = read_folder('./cwr_faulty/')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pdf = pd.DataFrame(result_faulty)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pdf.to_csv('result_faulty_pandas.csv', header=False, index=True)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_healhty = spark.read.csv('result_healthy_pandas.csv')\ndf_healhty.createOrReplaceTempView('df_healhty')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "spark.sql('select _c1,count(_c1) as cn from df_healhty group by _c1 order by cn asc').show()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_faulty = spark.read.csv('result_faulty_pandas.csv')\ndf_faulty.createOrReplaceTempView('df_faulty')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "spark.sql('select _c1,count(_c1) as cn from df_faulty group by _c1 order by cn asc').show()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import ibmos2spark\n\n# @hidden_cell\ncredentials = {\n    'auth_url': 'https://identity.open.softlayer.com',\n    'project_id': '6aaf54352357483486ee2d4981f8ef15',\n    'region': 'dallas',\n    'user_id': 'e8a574f22ee84d29b7a1987b6103fced',\n    'username': 'member_adcb54bd899a7e39e31582bccad1577f68f1992f',\n    'password': 'P*/m8,!#7s6H9poz'\n}\n\nconfiguration_name = 'os_d3bd5b94a9334de59a55a7fed2bedeaa_configs'\nbmos = ibmos2spark.bluemix(sc, credentials, configuration_name)\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_healhty.write.parquet(bmos.url('courseraai', 'cwr_healthy1.parquet'))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_faulty.write.parquet(bmos.url('courseraai', 'cwr_faulty1.parquet'))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df_faulty.count()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Keras\uc5d0\uc11c \ub3cc\ub9ac\uae30", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import ibmos2spark\n\n# @hidden_cell\ncredentials = {\n    'auth_url': 'https://identity.open.softlayer.com',\n    'project_id': '6aaf54352357483486ee2d4981f8ef15',\n    'region': 'dallas',\n    'user_id': 'e8a574f22ee84d29b7a1987b6103fced',\n    'username': 'member_adcb54bd899a7e39e31582bccad1577f68f1992f',\n    'password': 'P*/m8,!#7s6H9poz'\n}\n\nconfiguration_name = 'os_d3bd5b94a9334de59a55a7fed2bedeaa_configs'\nbmos = ibmos2spark.bluemix(sc, credentials, configuration_name)\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 16
        }, 
        {
            "source": "df_healthy = spark.read.parquet(bmos.url('courseraai', 'cwr_healthy.parquet'))\ndf_healthy.createOrReplaceTempView('df_healthy')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 17
        }, 
        {
            "source": "df_faulty = spark.read.parquet(bmos.url('courseraai', 'cwr_faulty.parquet'))\ndf_faulty.createOrReplaceTempView('df_faulty')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 18
        }, 
        {
            "source": "df_healthy = df_healthy.repartition(1)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 19
        }, 
        {
            "source": "df_healthy.write.csv('cwr_healthy2.csv')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 21
        }, 
        {
            "source": "df_faulty = df_faulty.repartition(1)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 22
        }, 
        {
            "source": "df_faulty.write.csv('cwr_faulty2.csv')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 24
        }, 
        {
            "source": "import pandas as pd\ndf_healthy = pd.read_csv('cwr_healthy2.csv/part-00000-c08ced3c-6ac7-4da9-be74-33077e7844b5.csv', engine='python', header=None)\ndf_healthy.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "error", 
                    "evalue": "[Errno 2] No such file or directory: 'cwr_healthy2.csv/part-00000-c08ced3c-6ac7-4da9-be74-33077e7844b5.csv'", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-28-85909fbc99e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_healthy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cwr_healthy2.csv/part-00000-c08ced3c-6ac7-4da9-be74-33077e7844b5.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_healthy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    703\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1053\u001b[0m                                  ' \"c\", \"python\", or' ' \"python-fwf\")'.format(\n\u001b[1;32m   1054\u001b[0m                                      engine=engine))\n\u001b[0;32m-> 1055\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m   2063\u001b[0m         f, handles = _get_handle(f, mode, encoding=self.encoding,\n\u001b[1;32m   2064\u001b[0m                                  \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2065\u001b[0;31m                                  memory_map=self.memory_map)\n\u001b[0m\u001b[1;32m   2066\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;31m# Python 3 and no explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;31m# Python 3 and binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cwr_healthy2.csv/part-00000-c08ced3c-6ac7-4da9-be74-33077e7844b5.csv'"
                    ], 
                    "ename": "FileNotFoundError"
                }
            ], 
            "execution_count": 28
        }, 
        {
            "source": "df_healthy.loc[df_healthy[1] == 100]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "error", 
                    "evalue": "'DataFrame' object has no attribute 'loc'", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-29-da99519f977c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_healthy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_healthy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    971\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             raise AttributeError(\n\u001b[0;32m--> 973\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m    974\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'loc'"
                    ], 
                    "ename": "AttributeError"
                }
            ], 
            "execution_count": 29
        }, 
        {
            "source": "import numpy as np\nfrom numpy import concatenate\nfrom matplotlib import pyplot\nfrom pandas import read_csv\nfrom pandas import DataFrame\nfrom pandas import concat\nimport sklearn\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.callbacks import Callback\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Activation\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport time\n%matplotlib inline", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "Using TensorFlow backend.\n"
                }
            ], 
            "execution_count": 30
        }, 
        {
            "source": "def get_recording(df,file_id):\n    return np.array(df.orderBy(df['_c0']).where(df['_c1'] == file_id).select('_c2','_c3').rdd.map(lambda row: np.array([row._c2,row._c3])).collect())", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 31
        }, 
        {
            "source": "def get_recording(df,file_id):\n    return np.array(df.sort_values(by=0, ascending=True).loc[df[1] == file_id].drop(0,1).drop(1,1))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 32
        }, 
        {
            "source": "from io import StringIO\nimport requests\nimport json\nimport pandas as pd\n\n# @hidden_cell\n# This function accesses a file in your Object Storage. The definition contains your credentials.\n# You might want to remove those credentials before you share your notebook.\ndef get_object_storage_file_with_credentials_d3bd5b94a9334de59a55a7fed2bedeaa(container, filename):\n    \"\"\"This functions returns a StringIO object containing\n    the file content from Bluemix Object Storage.\"\"\"\n\n    url1 = ''.join(['https://identity.open.softlayer.com', '/v3/auth/tokens'])\n    data = {'auth': {'identity': {'methods': ['password'],\n            'password': {'user': {'name': 'member_adcb54bd899a7e39e31582bccad1577f68f1992f','domain': {'id': '4619da2fa8524beda11c89d2d1969c5b'},\n            'password': 'P*/m8,!#7s6H9poz'}}}}}\n    headers1 = {'Content-Type': 'application/json'}\n    resp1 = requests.post(url=url1, data=json.dumps(data), headers=headers1)\n    resp1_body = resp1.json()\n    for e1 in resp1_body['token']['catalog']:\n        if(e1['type']=='object-store'):\n            for e2 in e1['endpoints']:\n                        if(e2['interface']=='public'and e2['region']=='dallas'):\n                            url2 = ''.join([e2['url'],'/', container, '/', filename])\n    s_subject_token = resp1.headers['x-subject-token']\n    headers2 = {'X-Auth-Token': s_subject_token, 'accept': 'application/json'}\n    resp2 = requests.get(url=url2, headers=headers2)\n    return StringIO(resp2.text)\n\ndf_data_1 = pd.read_csv(get_object_storage_file_with_credentials_d3bd5b94a9334de59a55a7fed2bedeaa('courseraai', 'cwr_healthy.csv/part-00000-2fe5401d-6223-4931-880e-634da433b039.csv-attempt_20180202060424_0005_m_000000_0'))\ndf_data_1.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "Empty DataFrame\nColumns: [<html><h1>Not Found</h1><p>The resource could not be found.</p></html>]\nIndex: []", 
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>&lt;html&gt;&lt;h1&gt;Not Found&lt;/h1&gt;&lt;p&gt;The resource could not be found.&lt;/p&gt;&lt;/html&gt;</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
                    }, 
                    "execution_count": 33, 
                    "metadata": {}
                }
            ], 
            "execution_count": 33
        }, 
        {
            "source": "import numpy as np\nhealthy_sample = get_recording(df_healthy,100)\nfaulty_sample = get_recording(df_faulty,125)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "error", 
                    "evalue": "'DataFrame' object has no attribute 'sort_values'", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-34-09956011bbc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhealthy_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_healthy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfaulty_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_faulty\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m125\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m<ipython-input-32-e44aaf998beb>\u001b[0m in \u001b[0;36mget_recording\u001b[0;34m(df, file_id)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfile_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    971\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             raise AttributeError(\n\u001b[0;32m--> 973\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m    974\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'sort_values'"
                    ], 
                    "ename": "AttributeError"
                }
            ], 
            "execution_count": 34
        }, 
        {
            "source": "fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\nsize = len(healthy_sample)\nax.plot(range(0,size), healthy_sample[:,0], '-', color='red', animated = True, linewidth=1)\nax.plot(range(0,size), healthy_sample[:,1], '-', color='blue', animated = True, linewidth=1)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\nsize = len(faulty_sample)\nax.plot(range(0,size), faulty_sample[:,1], '-', color='red', animated = True, linewidth=1)\nax.plot(range(0,size), faulty_sample[:,0], '-', color='blue', animated = True, linewidth=1)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\nax.plot(range(0,500), healthy_sample[:500,0], '-', color='red', animated = True, linewidth=1)\nax.plot(range(0,500), healthy_sample[:500,1], '-', color='blue', animated = True, linewidth=1)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\nax.plot(range(0,500), faulty_sample[:500,0], '-', color='red', animated = True, linewidth=1)\nax.plot(range(0,500), faulty_sample[:500,1], '-', color='blue', animated = True, linewidth=1)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "class LossHistory(Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n\n    def on_batch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "timesteps = 100\ndim = 2\nlossHistory = LossHistory()\n# design network\n\nmodel = Sequential()\nmodel.add(LSTM(50,input_shape=(timesteps,dim),return_sequences=True))\nmodel.add(Dense(2))\nmodel.compile(loss='mae', optimizer='adam')\n\ndef train(data):\n    model.fit(data, data, epochs=20, batch_size=72, validation_data=(data, data), verbose=1, shuffle=False,callbacks=[lossHistory])\n\ndef score(data):\n    yhat =  model.predict(data)\n    return yhat", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "def create_trimmed_recording(df,file_id):\n    recording = np.array(df.orderBy(df['_c0']).where(df['_c1'] == file_id).select('_c2','_c3').rdd.map(lambda row: np.array([row._c2,row._c3])).collect())  \n    samples = len(recording)\n    trim = samples % 100\n    recording_trimmed = recording[:samples-trim]\n    recording_trimmed.shape = (samples/timesteps,timesteps,dim)\n    return recording_trimmed", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "def create_trimmed_recording(df,file_id):\n    recording = get_recording(df,file_id) \n    samples = len(recording)\n    trim = samples % 100\n    recording_trimmed = recording[:samples-trim]\n    recording_trimmed.shape = (samples/timesteps,timesteps,dim)\n    return recording_trimmed", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#pd.unique()\n#df_healthy.drop(0,1).drop(2,1).drop(3,1)\npd.unique(df_healthy.iloc[:,1])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "file_ids = pd.unique(df_healthy.iloc[:,1])\nstart = time.time()\nfor file_id in file_ids:\n    recording_trimmed = create_trimmed_recording(df_healthy,file_id)\n    print \"Staring training on %s\" % (file_id)\n    train(recording_trimmed)\n    print \"Finished training on %s after %s seconds\" % (file_id,time.time()-start)\n\nprint \"Finished job on after %s seconds\" % (time.time()-start)\nhealthy_losses = lossHistory.losses", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\nsize = len(healthy_losses)\nplt.ylim(0,0.008)\nax.plot(range(0,size), healthy_losses, '-', color='blue', animated = True, linewidth=1)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#file_ids = spark.sql('select distinct _c1 from df_healhty').rdd.map(lambda row : row._c1).collect()\nstart = time.time()\nfor file_id in [105]:\n    recording_trimmed = create_trimmed_recording(df_faulty,file_id)\n    print \"Staring training on %s\" % (file_id)\n    train(recording_trimmed)\n    print \"Finished training on %s after %s seconds\" % (file_id,time.time()-start)\n\nprint \"Finished job on after %s seconds\" % (time.time()-start)\nfaulty_losses = lossHistory.losses", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\nsize = len(healthy_losses+faulty_losses)\nplt.ylim(0,0.008)\nax.plot(range(0,size), healthy_losses+faulty_losses, '-', color='blue', animated = True, linewidth=1)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }
    }, 
    "nbformat": 4
}